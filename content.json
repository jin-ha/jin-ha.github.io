{"pages":[],"posts":[{"title":"pip(Pypi) 미러서버 설정","text":"데이터분석 공부를 위해 셋팅해둔 docker 이미지(jupyter)를 오랜만에 다시 build 하니 tensorflow 패키지 다운로드가 너무 오래 걸려서 build가 timeout이 되었다.Pypi(pip)의 mirror 설정으로 변경 후 다시 실행하니 바로 해결! mirror 서버 설정은 아래와 같이 ~/.pip/pip.conf을 국내 서버로 설정 ~/.pip/pip.conf123[global]index-url=http://ftp.daumkakao.com/pypi/simpletrusted-host=ftp.daumkakao.com Dockerfile의 경우 아래와 같이 .pip 디렉토리 생성후 동일한 설정 Dockerfile12RUN mkdir ~/.pipRUN printf \"[global]\\nindex-url=http://ftp.daumkakao.com/pypi/simple\\ntrusted-host=ftp.daumkakao.com\" &gt; ~/.pip/pip.conf","link":"/2020/04/19/etc/pip-mirror/"},{"title":"Github page 블로그 시작 (hexo icarus 설정)","text":"예전부터 미뤄왔던 블로그를 이제서야 시작해본다.첫글은 hexo icarus 테마 중 내가 몇가지 수정한 것을 포스팅 한글 font 변경 profile 변경 article 가독성을 위해 변경 그 외에도 몇가지 (방문자수 등) introhexo 관련 구글링을 하다보면 거의 모든 hexo 블로그의 시작 포스팅은 이 hexo 설정글이다.hexo 설치나 기본 포스팅 방법들은 구글링하면 잘 정리해둔 블로그들이 엄청 많으니 생략하고,나도 그 설정글 작성에 동참을 해보기 위해 몇가지 내가 수정한 사항들만 포스팅해본다. 수정한 내용 중 틀린부분이 있을 수 있습니다.혹시 틀린부분이나 더 정확한 수정방법이 있으면 댓글 남겨주세요. icarus 설정icarus 릴리즈노트를 보면 3.0 대로 버전이 올라가면서 레이아웃이 JSX과 Inferno.js으로 모두 변경되었다고 한다.icarus 설정글 구글링시 대부분 기존 html 템플릿 엔진인 ejs를 기준으로 작성된 블로그가 대부분이다..프론트엔드쪽을 많이 본적은 없지만 jsx 문법은 꽤나 직관적인것 같다. 아직은 몇가지 스타일만 변경하였는데 jsx도 조금씩 수정해봐야겠다. 이 글은 icarus 3.0.0 버전을 기준으로 작성합니다. 한글 font 변경icarus 기본 한글 글씨체도 나쁘지 않았지만, 나눔스퀘어라운드 폰트를 사용하기로 했다. 폰트 cdn import아래 cdn 주소를 import 해준다. themes/icarus/source/css/default.styl1@import url(https://cdn.rawgit.com/innks/NanumSquareRound/master/nanumsquareround.css) 폰트 지정기존 $family-sans-serif 라인을 주석처리 한뒤, 아래처럼 지정한다. themes/icarus/include/style/base.styl12// $family-sans-serif ?= Ubuntu, Roboto, 'Open Sans', 'Microsoft YaHei', sans-serif$family-sans-serif ?= 'NanumSquareRound', \"Helvetica Neue\", sans-serif profile 변경기존 profile 위젯의 하단은 무조건 follow 라는 github 링크 버튼이 따라다니는데, 뭔가 테마에 맞지 않는 파란색으로 느껴졌다. 그리고 포스팅 내용을 본 뒤 home으로 가는 버튼이 있으면 좋을것 같아 해당 위치의 버튼을 home 버튼기능으로 변경하고, 디자인도 변경하였다. icarus 테마는 Bulma라는 CSS 프레임웍을 사용한다.https://bulma.io/documentation/elements/button/#styles 에서 다양한 버튼이 확인 가능하다. themes/icarus/layout/widget/profile.jsx을 보면 기존 followLink 코드를 볼 수 있다. themes/icarus/layout/widget/profile.jsx123{followLink ? &lt;div class=\"level\"&gt;&lt;a class=\"level-item button is-primary is-rounded\" href={followLink} target=\"_blank\" rel=\"noopener\"&gt;{followTitle}&lt;/a&gt;&lt;/div&gt; : null} github follow 기능을 유지하고 버튼 스타일만 변경할 경우 위 코드의 class 부분을 bulma 링크에서 고른 원하는 스타일로 예) “level-item button is-info is-outlined is-rounded” 으로 변경 하면 되고, 기능도 home으로 변경하려면 기존의 위 코드를 주석 처리하고 아래의 코드를 넣어준다. 아래 코드로 변경하면 profile widget의 followLink가 아닌 홈(“/“) 경로로 static하게 고정된다. themes/icarus/layout/widget/profile.jsx123&lt;div class=\"level\"&gt;&lt;a class=\"level-item button is-info is-outlined is-rounded\" href=\"/\" target=\"_self\" rel=\"noopener\"&gt;Home&lt;/a&gt;&lt;/div&gt; article 가독성을 위해 변경포스팅의 가독성은 hexo Tranquilpeak 테마의 스타일이 좋은것 같았다. 해당 스타일의 css을 참고하여 나의 입맛에 맞추어 수정 하였다. 포스트 제목 폰트 변경 (&amp;.article 밑 .title이 없을텐데 2~5줄 코드를 추가) themes/icarus/include/style/article.styl12345&amp;.article .title font-size: 2.0em font-weight: 700 letter-spacing: -1.4px; 포스트 내용 폰트 변경 (.content 내용을 아래와 같이 수정/추가) themes/icarus/include/style/article.styl123456.content font-size: $article-font-size line-height: 1.9em; letter-spacing: -0.3px; color: #5d686f; margin-top: 20px; heading 폰트 변경 (h1과 h2 수정)heading 간격이 너무 붙어 있어서 조금 벌리고, h1, h2 폰트사이즈도 큰 차이가 없는것 같아서 수정 themes/icarus/include/style/article.styl12345678h1 font-size: 1.95em font-weight: 600 margin-top: 50px;h2 font-size: 1.5em margin-top: 35px; columns 사이즈 변경widescreen일 경우 포스팅내용의 좌우폭을 늘림, bulma의 columns 사이즈를 참고하여 기본 layout 사이즈를 1 높였다. 메뉴얼을 보면 항상 12 columns 를 맞추게 되어있는데 내 블로그의 경우 블로그 내용을 볼때는 위젯을 우측 한줄만 사용함으로 사이즈를 1 높이면, widgets.jsx 을 1 줄여야 할것 같은데 생각보다 위젯이 너무 좁아보여서 그냥 포스팅 내용 폭만 늘렸다. (css를 잘몰라서 사용해보다가 이상한 부분이있으면 다시 조정해야겠다.) 내 블로그는 글을 눌러서 볼때 columnsCount 가 2줄이므로 아래 7번째줄 코드만 is-9-widescreen으로 사이즈 1증가 themes/icarus/layout/layout.jsx123456789101112&lt;div class=\"columns\"&gt; &lt;div class={classname({ column: true, 'order-2': true, 'column-main': true, 'is-12': columnCount === 1, 'is-8-tablet is-8-desktop is-9-widescreen': columnCount === 2, 'is-8-tablet is-8-desktop is-6-widescreen': columnCount === 3 })} dangerouslySetInnerHTML={ __html: body }&gt;&lt;/div&gt; &lt;Widgets site={site} config={config} helper={helper} page={page} position={'left'} /&gt; &lt;Widgets site={site} config={config} helper={helper} page={page} position={'right'} /&gt;&lt;/div&gt; 그 외에도 몇가지그외에도 이것저것 수정을 했는데 다 기억이 나지 않는다..혹시 필요한 부분이 있다면 댓글 부탁드립니다. logo image 크기 늘림 navbar 의 메뉴간 사이 변경 Google Analytics는 계정생성 후 themes/icarus/_config.yml 에 tracking_id 추가시 적용 됨. 포스팅별 읽은 수 및 footer에 나오는 총 방문자는 busuanzi: true 만 적용하니 잘 적용 됨. 덴마크어 yml만 적용되어있어서 처음 변경시 덴마크어로 나오는데, themes/icarus/languages/ko.yml에 plugin: visit:과 visitor: 추가하여 한글추가 등 끝.","link":"/2020/04/18/etc/hexo-blog/"},{"title":"Splunk Custom Search Command 개발 - (1)","text":"Splunk 를 사용하면서 큰 장점 중 하나로 생각했던, Custom Search Command 에 대해 포스팅하고자 합니다. 해당 기능을 통해서는 아래와 같은 스플렁크 앱 개발은 물론 다양한 아이디어 적용이 가능합니다. Custom Search Command로 개발가능한 Splunk 앱의 예시1. ElasticSearch API를 이용하여 ES의 Data를 Splunk로 Integration (가져온 Data는 Splunk search-head 메모리에 올라왔기 때문에 stats 등 SPL은 물론 경보 및 레포트등 스플렁크 기능 활용 가능)2. Splunk 에 인덱싱된 Data를 내가만든 명령어 SPL으로 Streaming하여 원하는곳으로 전송3. 등등.. Python 코드로 작성가능한 모든것 (Splunk 로 가져오거나, 내보내거나.) 이번 포스팅은 아래와 같이 3개의 주제로 나눠서 게시될 예정입니다. Custom Search Command 란? - 이번 게시물 Kaggle dataset download 앱 개발해보기 Event별 _time 관리 / time range 입력받기 / 명령어 도움말 관리 intro저는 업무에서 다양한 Custom Search Command 을 만들어봤었습니다.제일 처음 만들어봤던 앱은 예시 1의 내용인 ElasticSearch API를 이용하여 ES의 Data를 Splunk로 Integration 하는 Command 였었습니다. 당시 저는 Splunk 의 라이센스 부족에 힘들어 하며…ElasticSearch로 부족한 인덱싱을 메우고 있었습니다. 하지만, 생각보다 ElasticSearch의 Alert 기능이나 실제 로그를 검색하는 사용자들의 Web UI 편의성 (Kibana.. Lucene.. Aggregation을 위해서는 DSL 까지..)이 떨어져서 고민을 하고있었는데, 그에 딱 들어맞는 해결책이 Splunk Custom Search Command 였습니다. 앱 개발을 통해 ElasticSearch에 (무료로) Indexing 된 데이터를 스플렁크로 고스란히 가져와 스플렁크의 강력한 통계 SPL들은 물론 모두 사용할 수 있었고 Dashboard 와 Alert 기능등도 활용하며, 필요하다면 특정 로그들은 스플렁크 summary index로 다시 collect 할수도 있었습니다. 늘어나는 로그량에 비해 부족한 Splunk 라이센스에 힘들어하던 Splunk 담당자... 일단 하나를 만들어보니 그 이후로는 필요한 명령어들은 쉽게 만들 수 있었습니다.스플렁크 기본기능으로는 힘든 문제도 Custom Search Command의 개발로 간단하게 해결할 수도 있었습니다. 아래와 같은 문제들을 해결 가능DB Connector(DBX)에 mongoDB 기능은 포함되어 있지않아서 mongoDB로 Splunk 데이터를 밀어넣는 앱을 개발한다거나,Slack 등 글로벌앱은 splunkbase에 잘 있지만 국내 메신저는 없어서 앱개발을 통해 간단히 Splunk 경보를 국내 메신저로 Notify 한다거나.스케쥴링에 의해 돌아가는 특정 검색조건의 결과 로그들에 한해서만 제 3의 저장소(Qradar)로 원하는 형식으로 전송한다거나....등 Splunk는 docs 와 community 가 잘 되어있기는 하지만, 그 당시 Custom Search Command는 비교적 부족한 부분도 있었던거 같습니다..(특히 App별 Python Package 관리 및 _time, _raw 등 internal field에 대한 정의의 설명 부족) 이 글은 보시는 분들도 아직 Custom Search Command를 개발해 사용해보시지 않았다면, 이 기능을 통해 더 넓은 Splunk의 활용에 도움이 되었으면 좋겠습니다. Custom Search Command 란?v1과 v2의 차이Custom Searach Command의 버전은 v1 protocol과 v2 protocol가 있습니다.단순하게 v1은 기존 버전 v2 개선된 버전이라고 보시면 됩니다.차이는 아래와 같습니다. v1 protocol초창기 버전, intersplunk.py 라는 이전 SDK 를 통해 개발합니다.Splunk 3.0 이후 부터 사용 가능합니다.대용량 데이터 처리를 위해서는 v2가 더 효율적입니다.python 에 대해서만 지원하고,간단한 명령어를 만들기에 적합합니다. v2 protocolSplunk 6.3 이후 부터 사용 가능합니다.대용량 데이터 처리에 있어 v1 보다 runtime 오버헤드가 크게 줄어들었다고 합니다.python 뿐만 아니라, Go, java, C++ 도 지원 한다고 합니다. (저는 파이썬으로만 개발해봐서 상세 내용은 모르겠네요.)v2의 경우 아래의 4가지의 타입의 Splunk 명령어를 미리 인터페이스화 해놨습니다.즉, 4가지 중 만들고자 하는 타입의 Class 를 상속받아 구현하면 됩니다. Command Type Command Examples GeneratingCommand makeresults, inputlookup, inputcsv… StreamingCommand eval, fields, rename, where… ReportingCommand chart, timechart, stats, top… EventingCommand sort, dedup… 각 타입에 매칭되는 SPL의 기본 Command Examples을 보시면, 각 타입이 어떤 역할을 하는 Command 인지 이해하시기 쉬우실 것 같습니다. Genarating : 말그대로 events 를 생성하고자 할때 사용합니다. 항상 SPL의 최초 파이프라인에서 실행되어야 합니다.Streaming : events들을 one-by-one 으로 처리(변환 등)를 하고자 할때 사용합니다.Repoting : Data table 형태로 만들어주고자 할때 사용합니다.Eventing : Data set가 필요하며, 변환등이 아닌 odering과 같은 처리를 하고자 할때 사용합니다. 내가 만들고자 하는 앱이 어떤 Type에 적합한지 확인하고,거기에 맞는 Class를 상속받아 실제 코드를 구현하기만 하면 됩니다. 또한 Configuration Class와 Option Class도 제공해주는데, 이는 위와 같은 Commands를 만들다 보면 추가적인 설정이 필요해짐을 느낄때 찾아보시면 될것 같습니다. (ex, Streaming Command 를 만드는데 특정 변수를 각 indexer 끼리 shared 하고자 할때 Congifuration 어노테이션에 local=True) AppApp 은 Dashboard, Alert, Command 등을 포함할 수 있습니다.저희가 만들 Custom Search Command 는 이 중 Command 부분입니다.Splunkbase 등에서 앱을 설치하면 특정 SPL Command가 생기듯이, 저희는 App 구조안에 Command 만 만들어 볼 예정 입니다. Custom Search Command 을 몇개 만들면서 사용해본 결과 하나의 App 안에 필요한 Command 등을 몰아넣고 배포하고 관리하는것이 더 편리하였습니다. 하나의 앱 안에 여러 Custom search command 들을 개발하고 관리하는 기준으로 설명하도록 하겠습니다. App 생성일단 빈 App을 하나 생성해보겠습니다.[Splunk] - [앱 관리] - [앱 만들기] - [기본적인 앱 이름 등 설정 후 템플릿은 barebones으로 만듭니다] $SPLUNK_HOME/etc/app/하단에 위에서 생성한 빈 App 이 생성 됩니다.저는 Mac 환경에 Test App 이라는 앱을 생성했습니다. App 구조생성된 빈 Test App의 구조는 아래와 같이 되어있습니다. 생성해야할 파일은 아래와 같습니다. bin/“각 명령어 python 파일” 위에서 설명한 Type에 맞는 클래스를 상속받아 구현해야할 Python 코드입니다. default/commands.conf [mycommand]filename = mycommand.pypython.version = python3chunked = true [mycommand] stanza가 SPL에서 사용된 명령어의 이름입니다. 즉 이렇게 작성하면 스플렁크에서 mycommand라는 명령어가 만들어지고, 이는 bin의 mycommand.py를 구동시켜 동작하게 됩니다. 한 App 안에 여러 명령어를 만들고 싶다면 [command] stanza를 이 파일에 추가해주면 됩니다. python 코드자체에 v1, v2을 체크하는 부분이 없기 때문에, commands.conf의 각 명령어 stanza에 chucked를 통해 v1과 v2를 정의해줘야 합니다. chunked = true 일 경우 v2 protocol 의 custom search command 입니다. (옵션) default/searchbnf.conf searchbnf.conf는 명령어의 사용법이나 example query등을 작성하여, Splunk 검색 길잡이에 표시되게 하는 설정 입니다. 잘 활용하면 사용자 편의성을 높일 수 있습니다.3번째 Tip 포스팅에서 자세한 내용을 다루도록 하겠습니다. 다음 포스팅에서는 바로 실제 캐글의 데이터셋을 손쉽게 다운 받을 수 있는 Generating Custom Search Command를 만들어보겠습니다.","link":"/2021/04/13/data/splunk/custom-search-command-1/"},{"title":"Splunk Custom Search Command 개발 - (2)","text":"이번 포스팅에서는 Splunk Custom Search Command 를 이용하여 실제 Kaggle 에서 Dataset 을 Splunk 로 가져오는 앱을 만들어 보겠습니다. 미리 완성될 Kaggle SPL의 옵션값들을 떠올려 보는것이 만들어 나갈 코드를 어떤식으로 개발할지 방향성을 잡기에 쉽습니다. 아마 Kaggle 앱은 아래와 같이 동작하면 좋을것 같습니다.| kaggle competition=&quot;titanic&quot; data=&quot;train.csv&quot; 또한 불러올 데이터셋이 time series 데이터일 경우를 대비하여,time_field=&quot;create_time&quot; time_format=&quot;%Y/%m/%d %H:%M:%S&quot;라는 필드도 옵션으로 넣어줄 수 있게 만들어 주면 좋을 것 같네요. intro데이터분석 공부와 Kaggle 도전도 목표에 있고, 만들어두면 언젠가(?) 도움이 될듯하여 그리고 GeneratingCommand에 적합한 예제가 될듯하여 이 앱을 만들어 보기로 했습니다. 앞선 포스팅에서 설명드린 protocol 중 v2 protocol을 이용하여 만들어 볼 예정입니다. 간단한 명령어는 v1 protocol로 만들어도 되긴 하는데, 이전 버전의 라이브러리라 그런지 부족한 부분들이 꽤 있습니다. (v1에서 기본으로 제공해주는 getKeywordsAndOptions 함수는 v1 commands 를 실행 시 넘어오는 keyword 와 option을 파싱해줘야 하는데 제대로 파싱이 안됩니다…) kaggle API 확인먼저 kaggle API 동작방식을 이해해야 python 코드로 구현할 수 있습니다. Kaggle API Documents에는 pip으로 다운받은 kaggle 명령을 cli 에서 실행시켜서 (kaggle datasets download -d [DATASET]) 데이터셋을 다운을 받는데, 저희는 파이썬 코드상에서 SDK를 이용하여 다운 받고자 합니다. Kaggle API의 github을 찾아보니, 다음과 같은 코드와 주석을 찾았습니다. https://github.com/Kaggle/kaggle-api/blob/master/kaggle/api/kaggle_api_extended.pykaggle-api_링크9899100101102103104105106107108109110111112113114115116117118119120121...config_dir = os.environ.get('KAGGLE_CONFIG_DIR') or os.path.join( expanduser('~'), '.kaggle')if not os.path.exists(config_dir): os.makedirs(config_dir)config_file = 'kaggle.json'config = os.path.join(config_dir, config_file)config_values = {}......def authenticate(self): \"\"\"authenticate the user with the Kaggle API. This method will generate a configuration, first checking the environment for credential variables, and falling back to looking for the .kaggle/kaggle.json configuration file. \"\"\"......for key, val in os.environ.items(): if key.startswith('KAGGLE_'): config_key = key.replace('KAGGLE_', '', 1).lower() config_data[config_key] = val... 코드를 훑어본 결과 Kaggle SDK 호출하기 위해서는 먼저 인증 단계(authenticate function)가 이루어 져야 하는데, KAGGLE_ 로 시작하는 환경변수에 kaggle.json의 path를 지정 하여 인증하거나 ~/.kaggle/kaggle.json 의 파일을 바로 인증 둘 중 하나의 방식으로 인증을 해야 하는 것을 알았습니다.2번 방식을 사용하기에는 스플렁크앱이 어떤 사용자로 구동 되는지 실제 환경마다 다를것이기 때문에 저희는 1번 방식인 environment을 통해 인증을 진행하는 방식으로 kaggle 코드를 작성해 보겠습니다. kaggle.json 다운로드먼저 kaggle 홈에서 자신의 api key 정보를 다운로드 합니다. 다운받은 kaggle.json 파일은 단순 text 파일로 자신의 계정 및 API token이 json 형태로 작성되어 있습니다. 해당 파일을 일단 앞선 포스팅에서 만든 Splunk App 경로에 conf 라는 디렉토리를 추가하여 저장합니다. App 셋팅앞선 포스팅의 App 구조에서 설명한대로 작성한다면 conf 와 lib 디렉토리는 없지만 이 두 디렉토리는 명령어를 관리함에 있어서 명령어별로 Python library 및 config 들을 관리하기 위한 디렉토리라고 보시면 됩니다. conf 디렉토리 : 명령어별로 사용되는 설정값이 저장될 디렉토리 lib 디렉토리 : 명령어별로 사용되는 python library 가 저장될 디렉토리 이 디렉토리들은 필수디렉토리는 아닙니다. 한 앱에서 여러 Custom Search Command를 효율적으로 관리하기 위해 각 명령어별로 python library 경로와 설정값들을 분리시켜 저장시키는 공간이라고 보시면 됩니다. 앞선 포스팅의 기본 디렉토리에 conf, lib 디렉토리가 추가된 App 구조는 다음과 같아 집니다. conf/kaggle.json : Kaggle에서 다운받은 API token 파일 default/commands.conf : [kaggle] stanza를 추가한 설정 파일 bin/my_kaggle.py : 우리가 실제로 작성해야할 Python code library 및 conf 셋팅저희는 bin/my_kaggle.py 에서 kaggle 모듈을 import 해야 합니다.(+ Custom Search Command 개발에는 splunk-sdk도 필요 합니다.) python library 관리Custom Search Command 앱을 몇개 만들어 보면서 pip 으로 다운받는 라이브러리들을 여러방식으로 관리해봤지만, 이 방식이 가장 깔끔했습니다. 각 명령어별로 lib/[my_command]/ 디렉토리를 만들고 해당 경로에 requirements.txt 를 관리하여 각각 library를 다운로드 받고, 각 bin/[my_command] 의 코드 최초에 해당 디렉토리를 sys.path로 등록 Custom Search Command 파이썬 라이브러리 깔끔하게 관리하기 저희는 kaggle 1.5.12 버전과 splunk-sdk 1.6.14을 사용할 예정이라, requirements.txt 에 버전을 작성하고pip3 install -r requirements.txt --target=$SPLUNK_HOME/etc/apps/kaggle_dataset/lib/kaggle/. 을 실행하여 아래와 같이 해당 –target 디렉토리에 kaggle 및 splunk-sdk와 dependency 라이브러리들을 다운받습니다. conf 관리또한 명령어별로 config 값들을 관리할 필요가 자주있는데, 이번 경우에는 Kaggle 사이트에서 다운받은 토큰파일인 kaggle.json 파일입니다. 해당 파일을 conf 디렉토리에 위치 시켜줍니다. 실제코드코드는 먼저 requirement 들을 다운받은 library 경로를 sys.path로 등록시켜주고, 위에서 kaggle github 코드를 분석한대로 authenticate가 구동되게 하기 위해 kaggle.json파일이 있는 conf 디렉토리를 os.environ['KAGGLE_CONFIG_DIR']에 등록시켜주면 됩니다. 즉, 이번 my_kaggle.py의 코드는 아래와 같이 시작이 됩니다. bin/my_kaggle.py 의 시작코드12345import syssys.path.insert(0, '../lib/kaggle')import osos.environ['KAGGLE_CONFIG_DIR'] = os.getcwd() + '/../conf'from kaggle.api.kaggle_api_extended import KaggleApi 예제 GeneratingCommand 코드먼저 실제 어떤식으로 동작하는지 직접 보기위해 단순한 GeneratingCommand 을 만들어 실행 시켜 보겠습니다. 예제코드 bin/my_kaggle.py12345678910111213141516171819202122232425import syssys.path.insert(0, '../lib/kaggle')import osos.environ['KAGGLE_CONFIG_DIR'] = os.getcwd() + '/../conf'from kaggle.api.kaggle_api_extended import KaggleApiimport timeimport jsonfrom splunklib.searchcommands import dispatch, GeneratingCommand, Configuration, Option, validators@Configuration()class GenerateKaggleCommand(GeneratingCommand): foo = Option(require=False, default='bar') def generate(self): for i in range(10): yield self.getEvents({'foo':self.foo + str(i)}) def getEvents(self, result): event = result event['_time'] = time.time() event['_raw'] = json.dumps(result) return eventdispatch(GenerateKaggleCommand, sys.argv, sys.stdin, sys.stdout, __name__) bin/my_kaggle.py을 위와같이 작성 한뒤 splunk 재시작 후 | kaggle을 실행시켜 봅니다.(최초 명령어를 셋팅할때만 splunk 재시작이 필요합니다. 이후 코드 수정시 재시작 불필요.) 만든 앱의 명령을 다른 앱에서도 실행시키기 위해서는, 권한설정을 해줘야 합니다. 코드는 간단합니다, 앞선 포스팅에서 설명드린대로 GeneratingCommand 을 상속받아 여기서는 generate라는 function만 정의해주면 되고, yield 처리된 값이 Splunk의 event가 됩니다. yield 를 json 형태로 넘기면 그대로 json의 key 값이 스플렁크의 필드명, json의 value 값이 해당 필드의 값이 됩니다. 여기서는 getEvent라는 함수를 추가하여 Splunk의 internal field(_time, _raw) 들을 채워주도록 하였습니다. 각 type별 상세설명은 여기 docs에서 확인 가능하며, 이 공식 example code 들을 보는 것도 도움이 되었습니다. Option도 직관적이지만 Configuration Class와 Option Class을 참조하시면 도움이 됩니다. 실제 my_kaggle.py 작성하기이제 우리가 할일은 Option 클래스로 필요한 옵션값(명령어 인자값)들을 받고, Generete 함수안에서 Kaggle 모듈을 이용해 필요한 Dataset만 가져와 yield 처리 해주면 됩니다. 코드 전체를 설명하지는 않겠습니다. 만들어진 코드는 아래와 같습니다. 완성된 bin/my_kaggle.py1234567891011121314151617181920212223242526272829303132333435363738import syssys.path.insert(0, '../lib/kaggle')import osos.environ['KAGGLE_CONFIG_DIR'] = os.getcwd() + '/../conf'from kaggle.api.kaggle_api_extended import KaggleApiimport timeimport jsonimport csvfrom splunklib.searchcommands import dispatch, GeneratingCommand, Configuration, Option, validators@Configuration()class GenerateKaggleCommand(GeneratingCommand): competition = Option(require=True) data = Option(require=True) def generate(self): try: api = KaggleApi() api.authenticate() api.competition_download_file(self.competition, self.data) with open(self.data, newline='') as csvfile: reader = csv.DictReader(csvfile) for row in reader: yield self.getEvents(row) finally: if os.path.exists(self.data): os.remove(self.data) def getEvents(self, result): event = result event['_time'] = time.time() event['_raw'] = json.dumps(result) return eventdispatch(GenerateKaggleCommand, sys.argv, sys.stdin, sys.stdout, __name__) Option Class으로 저희가 본 포스팅 시작할때 구상한 옵션값들 competition, data을 필수로 사용자가 입력하도록 합니다. require=True의 옵션일 경우 사용자가 해당 옵션값들을 미입력시 에러처리를 아래와 같이 제공해줍니다. generate 함수안에서는 kaggle API의 competition_download_file 을 이용하여 옵션값으로 전달받은 competition 이름과 dataset 이름으로 대상 csv 파일을 다운받아, json 형식으로 읽은 후 yield 처리 해주기만 하면 됩니다.(competition_download_file 함수는 파일을 current directory에 파일을 다운로드 하기 떄문에, 삭제하는 코드도 넣어줬습니다.) 완성된 kaggle 명령어완성된 kaggle 명령은 아래와 같이 동작합니다. competition 옵션값으로 넘겨줄 값은 실제 Kaggle 웹사이트의 다운받을 competition 링크로 들어가 data 탭을 눌러 아래그림의 1에 있는 부분을 넣어주면 되고, data 옵션값은 실제 csv 파일명(아래그림의 2)을 넘겨주면 됩니다. 그리고 kaggle API github page에 아래와 같은 안내글이 있었네요..Note: you will need to accept competition rules at https://www.kaggle.com/c/&lt;competition-name&gt;/rules각 competition 별로 있는 rules는 꼭 페이지에서 accept 해줘야 합니다.. API 상에 accept하는 함수가 있는지 찾아봤는데 안보이네요. 아마 dataset 중 csv가 아닌 json도 있을 것입니다. 이는 옵션값 [type=csv|json]으로 코드상에서 해당 타입을 처리해주는 분기부분만 추가하면 될것 같은데, 필요하신분은 추가 개발을 해보시길 바랍니다. 지금은 해당 명령을 수행할 때 마다 kaggle API을 이용하여 다운로드를 수행하게됩니다.매번 명령을 수행할때 마다 다운로드 받아서 splunk에 밀어넣는 작업은 비효율적임으로, 이런 데이터들은 outputlookup을 통해 한번 가져 온 이후 그 다음부터는 API 호출없이 inputlookup을 통해 바로 이미 가져온 데이터를 사용하는게 효율적일 것입니다. 앞서 설명드렸듯이 이미 Custom Search Command로 가져온 데이터들은 모든 SPL을 활용 가능합니다. (outputlookup, collect, stats 등등) 끝.이상으로 kaggle 에서 dataset을 다운받아오는 GeneretingCommand를 만들어봤습니다. 다음 포스팅에서는 이 kaggle 명령을 time이 있는 데이터셋을 가져올 수 있도록 변경해보고 몇가지 팁들을 작성하도록 하겠습니다.","link":"/2021/04/13/data/splunk/custom-search-command-2/"},{"title":"Logstash Java filter plugin 개발","text":"Elastic 공식 Docs의 How to write a Java filter plugin 에 잘 나와있지만,내용대로 진행시 몇가지 에러가 발생했다. FAILURE: Build failed with an exception error: package co.elastic.logstash.api does not exist 으로 검색하여 들어오신 분에게 도움이 되었으면 합니다! intro이때까지 Logstash 를 사용하면서 기본 input, filter, output 플러그인들만 사용했었는데급하게 특정 필드의 암호화/복호화가 필요하여 구글링결과 Java filter plugin 라는 것을 찾았다. 해당 플러그인을 통해 아래와 같이 input 으로 들어온 특정 필드를 내가 만든 Java 코드를 통해 변환시킬 수 있다. Java filter plugin 을 통해 개발한 custom 암복호화 filter123456789101112input { ...}filter { my_decrypt_plugin { source =&gt; \"encrypted_field\" key =&gt; \"password\" }}... 이 문서에서는 Docs의 예제 코드를 plugin으로 만들때 발생하는 에러 해결방법을 설명합니다. Java filter plugin전체적인 방식은 How to write a Java filter plugin 에 잘 나와있지만,해당 내용대로 진행시 (특정 Logstash 버전에서는) 몇가지 에러가 발생한다.. Logstash 다운로드다음 방법 중 환경에 맞게 Logstash 를 다운로드Elastic - Download Logstash Logstash 버전 확인123$ bin/logstash --versionUsing bundled JDK: /usr/share/logstash/jdklogstash 7.11.2 Logstash-core 가 있어야 filter-plugin 빌드 가능Logstash-core compile123456789101112131415# 나에게 맞는 버전의 &lt;branch_name&gt; 으로 다운# git clone --branch &lt;branch_name&gt; --single-branch https://github.com/elastic/logstash.git &lt;target_folder&gt;cd ~/myworkmkdir logstash_corecd logstash_coregit clone --branch 7.11 --single-branch https://github.com/elastic/logstash.git .# 빌드./gradlew assemble# core 빌드 확인ll logstash-core/build/libs/-rw-r--r-- 1 root root 1349807 Feb 26 02:52 logstash-core-7.11.2-javadoc.jar-rw-r--r-- 1 root root 437112 Feb 26 02:52 logstash-core-7.11.2-sources.jar-rw-r--r-- 1 root root 645893 Feb 26 02:52 logstash-core-7.11.2.jar logstash-filter-java 예제코드 다운로드logstash-filter-java1234567cd ~/myworkgit clone https://github.com/logstash-plugins/logstash-filter-java_filter_example# elastic Docs에 있는대로 gradle.properties 만들어서 한줄추가# LOGSTASH_CORE_PATH=/root/mywork/logstash_core/logstash-core/# Docs 의 내용대로 다운받은 Java 코드를 원하는 필터의 형태로 수정해준다. 에러발생이제 이 내용대로 예제코드들을 gem으로 만들려고 ./gradlew gem 하면 에러 발생 첫번째 에러첫번째 에러12345678910FAILURE: Build failed with an exception.** Where:*Build file '/root/mywork/logstash-filter-java_filter_example/build.gradle' line: 102** What went wrong:*A problem occurred configuring root project 'logstash-filter-java_filter_example'.&gt; Could not create task ':gem'.&gt; No signature of method: org.gradle.api.internal.tasks.DefaultTaskDependency$TaskDependencySet.getAt() is applicable for argument types: (ArrayList) values: [[task ':downloadAndInstallJRuby', task ':removeObsoleteJars', ...]]# ... blah-blah 구글링결과 위와 같은 에러는 많은 글이 있었다.출처: https://discuss.elastic.co/t/logstash-java-filter-plugin-example-gradlew-gem-error/257110의 내용대로 build.gradle 을 수정 build.gradle1234dependsOn [downloadAndInstallJRuby, removeObsoleteJars, vendor, generateRubySupportFiles]을dependsOn ([downloadAndInstallJRuby, removeObsoleteJars, vendor, generateRubySupportFiles])으로 수정시 에러 해결 두번째 에러두번째 에러123error: package co.elastic.logstash.api does not existimport co.elastic.logstash.api.Configuration; ^ 패키지를 못찾는 에러 발생여기저기 구글링 중 결국 중국 블로그에서 해결책을 찾았다.출처: https://www.cnblogs.com/shiqi17/p/14168550.html build.gradle1234implementation fileTree(dir: LOGSTASH_CORE_PATH, include: \"**/logstash-core-?.?.?.jar\")을implementation fileTree(dir: LOGSTASH_CORE_PATH, include: \"**/logstash-core-?.??.?.jar\")으로 수정시 에러 해결 원인은 내가 받은 Logstash의 마이너 버전이 7.11.2 으로 두자리 숫자라 제대로 맵핑이 안되서 발생된 것이었다.Logstash 의 버전을 확인하여 마이너 버전이 두자리일 경우에만 build.gradle 의 위 코드 ?.?.? 을 ?.??.? 으로 수정 하면 됩니다. gem 으로 빌드된 custom plugin을 Logstash에 설치Java filter plugin install1234$ bin/logstash-plugin install --no-verify --local /root/mywork/logstash-filter-java_filter_example/logstash-filter-java_filter_example-1.0.2.gem...Installing logstash-filter-java_filter_exampleInstallation successful 설치한 플러그인 실행example plugin test123456789101112131415161718192021$ cat java_filter.confinput { generator { message =&gt; \"Hello world!\" count =&gt; 1 }}filter { java_filter_example {}}output { stdout { codec =&gt; rubydebug }}$ bin/logstash -f ./java_filter.conf{ \"message\" =&gt; \"!dlrow olleH\", \"@version\" =&gt; \"1\", \"@timestamp\" =&gt; 2021-02-26T04:05:27.633Z, \"host\" =&gt; \"host\", \"sequence\" =&gt; 0}","link":"/2021/03/11/data/elastic/logstash-java-filter-plugin/"},{"title":"Splunk Custom Search Command 개발 - (3)","text":"이번 포스팅에서는 마지막으로 Custom Search Command를 좀 더 효율적으로 사용할 수 있는 방법을 공유드릴 수 있도록 하겠습니다. _time 필드 활용하기 time range 입력받기 searchbnf 로 명령어 도움말 추가하기 기타.. _time 필드 활용하기앞서만든 kaggle 코드에서는 splunk event의 internal field 중 하나인 _time을 단순하게 time.time()으로 인덱싱되는 시점의 epoch time을 넣어줬습니다. 물론 time 값이 없는 dataset일 경우에는 _time은 필요 없겠지만 time series data인 경우 해당 timestamp 필드를 그대로 _time으로 매칭시켜주면 좋습니다. timechart 같은 splunk 명령은 _time를 기반으로 동작하는데 데이터를 genereting 할때부터 잘 파싱하여 수집한다면, 추가 작업없이 _time을 활용할 수 있기 때문입니다. 코드 추가각 competition의 dataset 마다 time field의 format은 제각각일 것임으로, 옵션을 주어서 사용자가 동적으로 format을 입력할 수 있도록 하겠습니다. 추가 옵션 2개인 time_field, time_format 받을 수 있도록 (단, require=False로 필수는 아니게) 하였고, 옵션이 있을 경우에만 strptime으로 timestamp를 파싱한 후 epoch time 으로 _time 필드에 넣어주었습니다. timestamp 파싱 부분 추가1234567891011121314151617181920212223242526272829303132333435363738394041424344import syssys.path.insert(0, '../lib/kaggle')import osos.environ['KAGGLE_CONFIG_DIR'] = os.getcwd() + '/../conf'from kaggle.api.kaggle_api_extended import KaggleApiimport timeimport jsonimport csvfrom calendar import timegmfrom splunklib.searchcommands import dispatch, GeneratingCommand, Configuration, Option, validators@Configuration()class GenerateKaggleCommand(GeneratingCommand): competition = Option(require=True) data = Option(require=True) time_field = Option(require=False, default='') time_format = Option(require=False, default='') def generate(self): try: api = KaggleApi() api.authenticate() api.competition_download_file(self.competition, self.data) with open(self.data, newline='') as csvfile: reader = csv.DictReader(csvfile) for row in reader: yield self.getEvents(row) finally: if os.path.exists(self.data): os.remove(self.data) def getEvents(self, result): event = result if self.time_field == '': event['_time'] = time.time() else: event['_time'] = timegm(time.strptime(event[self.time_field], self.time_format)) event['_raw'] = json.dumps(result) return eventdispatch(GenerateKaggleCommand, sys.argv, sys.stdin, sys.stdout, __name__) 결과 _time가 추가되면 스플렁크 검색시 이벤트탭의 시간 표시줄에도 정상적으로 표시되며, 아래 이벤트도 정상적으로 시간이 파싱되어 들어간 것을 볼 수 있습니다.(epoch time은 GMT로 잘 파싱되었는데, 스플렁크 계정설정이 로컬타임존에 맞게 설정되어 +0900 으로 보임) timechart도 정상적으로 동작함을 볼 수 있습니다. (앞 포스팅에서 설명드린대로 lookup으로 저장 후 호출 한 모습) time range 입력받기어떤 Custom Search Command 들은 time range값을 사용자에게서 입력받아야 할때가 있습니다. 앞서 만든 Kaggle 명령은 굳이 사용자에게서 time range를 전달받을 필요가 없었습니다.하지만 예를들어 제3의 Database에서 특정 time range 안의 데이터들만을 쿼리하여 스플렁크로 가져오는 GeneratingCommand를 만들고자 한다면 그 특정 time range를 사용자에게서 입력받아야 합니다. 옵션을 통해서 time range을 일일히 입력받을 수도 있겠지만 이는 비효율적입니다. (사용자 입장에서도 정해진 timeformat에 맞춰 일일히 입력해야해서 번거롭고, 명령어 개발하는 입장에서도 파싱이 번거럽고) 또한 다른 기본 SPL들도 공통적으로 아래의 방식으로 time range를 입력 받을 수 있도록 구성되어 있는데, 저희가 만드는 명령어 또한 통일시켜 주어야 사용자입장에서는 저희가 만든 command를 더 효율적으로 사용할 수 있을 것입니다. 이 방식으로 command를 만든다면, 다른 SPL과 동일하게 다음과 같이 time range를 사용할 수 있습니다. Splunk Web UI 상의 Time picker를 활용할 수 있다. Splunk REST API 로 만든 명령어 호출시 earliest_time, latest_time parameter도 활용 할 수 있다.-1d@d 같은 문법도 활용 가능합니다. 방법은 간단합니다.상속받은 각 Type의 Class에 있는 아래 속성값을 활용하면 됩니다. 상속받은 class로 넘어오는 time range 값12self.search_results_info.search_et = 'ealiest_time 이 전달되는 변수'self.search_results_info.search_lt = 'latest_time 이 전달되는 변수' 즉, 위 코드대로 상속받은 각 Type의 Class의 해당 속성값에 접근한다면 Web UI의 time picker와 REST API에서 지정한 ealiest_time과 latest_time에 손쉽게 접근할 수 있습니다.값은 milliseconds가 포함된 epoch time 일텐데, 이를 사용하여 제 3의 Database에 쿼리를 시간조건을 주어 질의하는 등 활용 가능하게 됨으로 손쉽게 사용자로부터 time range를 입력 받을 수 있습니다. searchbnf 작성기본 SPL들은 아래와 같이 검색길잡이(명령어 도움말)이 잘 작성되어 있습니다. 저희가 만든 앱도 검색길잡이의 문구를 넣어 example 명령어와 사용법등을 제시해준다면 사용자들이 더 편하게 사용할 수 있을 것입니다. default/searchbnf.conf 에 아래와 같이 작성하면 됩니다.적용하려면 splunk 재시작 필요 default/searchbnf.conf12345678910111213[kaggle-command]syntax = | kaggle COMPETITON=&lt;competition_name&gt; DATA=&lt;data_name&gt; [TIME_FIELD=&lt;field&gt;] [TIME_FORMAT=&lt;format&gt;]shortdesc = kaggle dataset을 splunk로 가져옵니다. \\ COMPETITON에 kaggle의 대회명을 DATA에 실제 csv 파일명을 입력해주세요. (필수 파라미터) \\description = time series data인 경우 TIME_FIELD에 실제 csv의 time이 있는 field명과 \\ TIME_FORMAT에 해당 field의 포맷을 입력시 각 row의 시간값이 splunk의 _time 값이 됩니다.comment1 = kaggle titanic 문제의 test.csv 파일을 가져옵니다.example1 = \\ | kaggle competition=titanic data=test.csvcomment2 = kaggle bike sharing 문제의 test.csv 파일을 datetime field를 _time 기준으로 가져옵니다.example2 = \\ | kaggle competition=bike-sharing-demand data=test.csv time_field=datetime time_format=&quot;%Y-%m-%d %H:%M:%S&quot;usage = public 그외 searchbnf.conf는 alias, tags 등과 같은 옵션도 있으니 참고바랍니다. 기타여기서는 몇가지 기억에 나는 내용을 공유드리겠습니다. run_in_preview 옵션streaming command를 만들때였는데, 스플렁크에 인덱싱된 데이터를 특정메신저로 notify해주는 명령이였습니다.웹에서 만든명령을 구동 시킬 때 가끔씩 2번씩 메신저로 전송이 되는 이슈가 있었는데, 결론은 commands.conf의 run_in_preview 옵션 때문이였습니다. 디폴트 값은 True이고, 결과들을 final output 하기전에 사용자에게 preview 해주기위한 옵션입니다. 이를 False로 셋팅을 해줌으로 2번씩 명령어코드가 실행되는 것을 해결할 수 있었습니다. 끝.이상 Cumstom Search Command 포스팅을 마치도록 하겠습니다.아직 해당 기능을 활용해 보시지 않았다면, 이 포스팅이 도움이 되었으면 좋겠습니다.","link":"/2021/04/13/data/splunk/custom-search-command-3/"}],"tags":[{"name":"python","slug":"python","link":"/tags/python/"},{"name":"pip","slug":"pip","link":"/tags/pip/"},{"name":"docker","slug":"docker","link":"/tags/docker/"},{"name":"hexo","slug":"hexo","link":"/tags/hexo/"},{"name":"splunk","slug":"splunk","link":"/tags/splunk/"},{"name":"develop","slug":"develop","link":"/tags/develop/"},{"name":"elastic","slug":"elastic","link":"/tags/elastic/"},{"name":"logstash","slug":"logstash","link":"/tags/logstash/"}],"categories":[{"name":"others","slug":"others","link":"/categories/others/"},{"name":"data","slug":"data","link":"/categories/data/"},{"name":"splunk","slug":"data/splunk","link":"/categories/data/splunk/"},{"name":"elastic","slug":"data/elastic","link":"/categories/data/elastic/"}]}